# Tier 3 Implementation Playbook #3: Co-Design & Citizen Engagement

## PPT Investment Breakdown

This playbook follows the People-Process-Technology (PPT) framework with a strong emphasis on people and process, reflecting the human-centered nature of co-design.

**Investment Allocation:**
- **People (50%):** $1-2.5M - User researchers, designers, facilitators, citizen engagement specialists, accessibility experts
- **Process (40%):** $0.8-2M - Co-design methodology, research protocols, design standards, accessibility guidelines, feedback loops
- **Technology (10%):** $0.2-0.5M - Research tools, prototyping platforms, collaboration software, analytics

**Total Investment:** $2-5M annually

This allocation reflects that co-design is fundamentally about people and process, not technology. The majority of investment goes to skilled researchers and designers who can facilitate meaningful citizen engagement, with technology serving as an enabler rather than the focus.

---

## Executive Summary

This playbook provides comprehensive guidance for implementing user-centered co-design processes in government digital services, aligned with the Government as a Service (GaaS) framework. Co-design ensures services are built with citizens, not just for them.

**Key Objectives:**
- Establish systematic co-design methodology across government
- Engage 1,000+ citizens in service design within 12 months
- Map 10+ life-event journeys
- Achieve 4.5+/5.0 citizen satisfaction for co-designed services
- Build culture of user-centered design across government
- Ensure accessibility (WCAG 2.1 AA compliance)

**Expected Outcomes:**
- 50%+ reduction in service redesign cycles
- 80%+ first-time completion rate for digital services
- 40%+ reduction in support tickets
- Citizen satisfaction >4.5/5.0 (vs. <3.5 for non-co-designed)
- Foundation for continuous service improvement

**Timeline:** Ongoing (with 6-month setup phase)
**Team Size:** 10-15 FTEs (Co-Design Center of Excellence)

---

## Co-Design Methodology

### **Framework: UK GDS Service Standard (Adapted)**

**16 Service Standard Criteria:**

1. **Understand user needs** - Research with users, not assumptions
2. **Solve a whole problem** - Design around life events, not bureaucratic structures
3. **Provide joined-up experience** - Seamless across channels and agencies
4. **Make service simple to use** - Minimize cognitive load, intuitive design
5. **Make sure everyone can use service** - Accessibility (WCAG 2.1 AA)
6. **Have a multidisciplinary team** - Diverse skills: design, research, development, content
7. **Use agile ways of working** - Iterate based on user feedback
8. **Iterate and improve frequently** - Launch early, improve continuously
9. **Create a secure service** - Security by design, build user trust
10. **Define what success looks like** - Measurable KPIs, track progress
11. **Choose the right tools and technology** - User needs drive tech choices, not reverse
12. **Make new source code open** - Default to open (with security exceptions)
13. **Use and contribute to open standards** - Interoperability, avoid lock-in
14. **Operate a reliable service** - 99.9%+ uptime, performance monitoring
15. **Test the service with minister** - Executive-level sign-off after testing
16. **Make service accessible** - WCAG 2.1 AA minimum, assistive technology support

**Governance:**
- All major services must pass Service Standard assessment before launch
- Independent assessment panel (include user research experts, accessibility specialists)
- Public reporting of assessment results (transparency)

---

### **Design Thinking Process (5 Stages)**

```
1. EMPATHIZE → 2. DEFINE → 3. IDEATE → 4. PROTOTYPE → 5. TEST
     ↓              ↓            ↓           ↓            ↓
  Research     Insights      Ideas      Mockups     Validate
     ↑______________________________________________|
                    (Iterate based on feedback)
```

**Stage 1: Empathize (Research)**
- **Methods:** Interviews, surveys, observation, diary studies
- **Participants:** 20-50 users (represent diversity of population)
- **Output:** User insights, pain points, needs, behaviors
- **Duration:** 2-4 weeks

**Stage 2: Define (Synthesize)**
- **Methods:** Affinity mapping, persona development, journey mapping
- **Output:** Problem statements, personas, current-state journey maps
- **Duration:** 1-2 weeks

**Stage 3: Ideate (Generate Solutions)**
- **Methods:** Brainstorming, design studios, co-creation workshops
- **Participants:** Users + designers + developers + policy experts
- **Output:** 10-20 concept ideas, shortlist to 2-3 for prototyping
- **Duration:** 1-2 weeks

**Stage 4: Prototype (Build)**
- **Methods:** Paper prototypes, clickable wireframes, functional prototypes
- **Fidelity:** Start low (sketches), increase (interactive mockups)
- **Output:** Testable prototypes
- **Duration:** 1-3 weeks

**Stage 5: Test (Validate)**
- **Methods:** Usability testing, A/B testing, pilot rollout
- **Participants:** 10-20 users per test round
- **Output:** Validation or iteration (what works, what doesn't)
- **Duration:** 1-2 weeks per round

**Iteration:** Repeat stages 4-5 multiple times before final build.

---

## Workshop Facilitation Guides

### **Workshop 1: Discovery Workshop (Kickoff)**

**Objective:** Align stakeholders on project goals, users, and approach

**Participants:**
- Service owner (agency representative)
- Policy experts
- Technical leads
- User researchers
- Designers
- 2-3 "super users" (citizens who will use service)

**Duration:** Half-day (4 hours)

**Agenda:**

**1. Welcome & Introductions (15 min)**
- Icebreaker: Each person shares one frustration with current service
- Set ground rules: All ideas welcome, no hierarchy, listen actively

**2. Project Overview (30 min)**
- Service owner presents: Current service, pain points, goals
- Q&A

**3. User Needs Exercise (45 min)**
- Brainstorm: "Who are our users?" (sticky notes on wall)
- Group into user segments (e.g., seniors, working adults, disabled, non-English speakers)
- For each segment: "What do they need? What are their barriers?"

**4. Journey Mapping (Current State) (60 min)**
- Map current user journey (from awareness to completion)
- Identify pain points at each step (mark with red sticky notes)
- Highlight moments of delight (green sticky notes) - usually few!

**5. Success Definition (30 min)**
- "How will we know we've succeeded?" - Define 3-5 measurable KPIs
  - Example: "80% complete service in one sitting" or "Satisfaction >4.0/5.0"

**6. Research Planning (30 min)**
- Who do we need to talk to? (user segments, sample size)
- What questions do we need to answer?
- What methods? (interviews, surveys, observation)
- Timeline and responsibilities

**7. Next Steps & Close (10 min)**
- Recap decisions
- Assign action items
- Schedule next session

**Facilitator Tips:**
- Use visual tools (large paper, sticky notes, markers)
- Timebox discussions (use timer)
- Park off-topic discussions ("parking lot" poster)
- Ensure all voices heard (ask quiet participants directly)
- Document everything (photos of walls, detailed notes)

---

### **Workshop 2: Personas & Journey Mapping**

**Objective:** Synthesize research into personas and ideal future journey

**Prerequisites:** User research completed (20+ interviews, survey with 200+ responses)

**Participants:** Core team (researchers, designers, service owner)

**Duration:** Full day (8 hours)

**Agenda:**

**1. Research Findings Presentation (60 min)**
- User researchers present key insights (themes, quotes, data)
- Affinity mapping: Group insights into themes on wall

**2. Persona Development (90 min)**
- Create 3-5 personas (represent key user segments)
- For each persona:
  - Name, photo (stock or illustration), demographics
  - Goals: What do they want to achieve?
  - Frustrations: What blocks them today?
  - Behaviors: How do they currently solve this problem?
  - Quote: Memorable statement that captures their perspective

**Example Persona:**
```
Name: Maria Santos
Age: 42, Small Business Owner
Quote: "I don't have time to navigate complicated government websites during work hours."

Goals:
- Renew business license quickly
- Understand tax obligations
- Access support programs for small businesses

Frustrations:
- Different websites for each agency (no single entry point)
- Operating hours during work day (can't take time off)
- Confusing jargon and forms

Behaviors:
- Uses smartphone for most tasks (rarely laptop)
- Asks accountant for help with government tasks (additional cost)
- Delays renewals until last minute (because it's painful)

Tech Comfort: Moderate (uses apps, not expert)
Accessibility Needs: None
```

**3. Lunch Break (60 min)**

**4. Journey Mapping (Future State) (180 min)**
- For primary persona, map ideal future journey:
  - **Awareness:** How do they discover the service?
  - **Consideration:** How do they decide to use it?
  - **Access:** How do they start the process?
  - **Completion:** Steps to achieve goal
  - **Follow-up:** What happens after?

- For each step, define:
  - **User actions:** What does user do?
  - **Touchpoints:** Website, app, phone, in-person?
  - **Emotions:** How does user feel? (use emoticons)
  - **Pain points:** What could go wrong?
  - **Opportunities:** How can we make this delightful?

**5. Prioritization (60 min)**
- Identify highest-impact improvements (impact vs. effort matrix)
- Agree on must-have features for MVP (minimum viable product)

**6. Wrap-up (30 min)**
- Finalize personas and journey maps
- Assign design and prototyping tasks

**Deliverables:**
- 3-5 personas (printed posters, shared digitally)
- Future-state journey map (visual diagram)
- Prioritized feature list

---

### **Workshop 3: Co-Creation (Ideation)**

**Objective:** Generate and refine service design ideas with users

**Participants:**
- 10-15 citizens (matching personas)
- Core team (designers, researchers, developers)
- Service owner

**Duration:** Half-day (4 hours)

**Agenda:**

**1. Welcome & Ice Breaker (15 min)**
- Introduce participants (first names only, privacy)
- Explain purpose: "Your ideas will shape this service"
- Icebreaker: "Share one time government service surprised you (good or bad)"

**2. Context Setting (20 min)**
- Present current service and pain points (using research findings)
- Show personas and journey map: "Do these resonate with your experience?"
- Set challenge: "How might we make this service simple, fast, and delightful?"

**3. Ideation Round 1: Crazy 8s (30 min)**
- **Method:** Each person sketches 8 different ideas in 8 minutes (1 min per idea)
- **Rule:** No idea too crazy, focus on quantity not quality
- **Share:** Pin all sketches on wall, silent gallery walk (no critique yet)

**4. Ideation Round 2: Group Brainstorming (45 min)**
- Divide into 3 groups (mix citizens and team members)
- Each group focuses on one part of journey (e.g., registration, payment, confirmation)
- Brainstorm on large paper: "How could we make this step amazing?"
- **Tools:** Sketches, sticky notes, annotations

**5. Break (15 min)**

**6. Concept Presentation (30 min)**
- Each group presents their ideas (5 min per group)
- Dot voting: Everyone gets 3 votes for favorite ideas (place dot stickers)
- Discuss top 5 ideas

**7. Rapid Prototyping (60 min)**
- Divide into teams, each builds paper prototype of top idea
- **Materials:** Paper, markers, scissors, tape
- **Output:** Sketch of screens/pages, user flow
- Teams present prototypes (3 min each)

**8. Feedback & Prioritization (30 min)**
- Citizen participants give feedback on each prototype
- Vote on which to develop further
- Capture suggestions for refinement

**9. Thank You & Next Steps (15 min)**
- Thank participants (provide compensation: gift card, recognition)
- Explain next steps: "We'll refine these ideas and invite you to test"
- Collect contact info for future testing

**Facilitator Tips:**
- Encourage wild ideas early (divergent thinking)
- Manage time strictly (use visible timer)
- Balance citizen and team voices (ensure citizens speak more)
- Capture everything (photos, notes, videos)
- Create safe space (no wrong answers, all ideas valued)

---

## User Research Techniques

### **1. In-Depth Interviews**

**When to Use:** Understand individual experiences, motivations, pain points

**Sample Size:** 15-30 interviews (until themes repeat - "saturation")

**Duration:** 45-60 minutes per interview

**Participant Recruitment:**
- Diverse demographics (age, income, education, geography, disability status)
- Mix of experienced users and non-users
- Screening survey to select representative sample

**Interview Guide Structure:**

**1. Introduction (5 min)**
- Thank you for participating
- Purpose: Improve government service
- Confidential, no right/wrong answers
- Permission to record (audio only)

**2. Background (10 min)**
- Tell me about yourself (family, work, where you live)
- How do you usually interact with government? (online, phone, in-person)
- How comfortable are you with technology?

**3. Current Experience (20 min)**
- Walk me through the last time you used [service]
- What prompted you to use it?
- What steps did you take?
- What was easy? What was frustrating?
- How long did it take?
- Did you complete it? If not, why?
- How did you feel during the process?

**4. Needs & Expectations (10 min)**
- What would make this service easier for you?
- If you could change one thing, what would it be?
- How do you prefer to interact with government? (channel preference)
- What information do you wish you had?

**5. Prototype Reaction (if available) (10 min)**
- Show early prototype or sketches
- What do you think this is?
- What would you do here?
- What questions do you have?
- What's missing?

**6. Closing (5 min)**
- Is there anything else you'd like to share?
- Thank you, compensation (gift card)
- Invite to future testing

**Analysis:**
- Transcribe interviews
- Code themes (affinity mapping)
- Identify patterns across participants
- Extract quotes for personas and presentations

---

### **2. Surveys**

**When to Use:** Quantify trends, reach large sample, validate hypotheses

**Sample Size:** 200-1,000+ respondents (depends on population size)

**Survey Design Best Practices:**
- **Short:** 10-15 questions (complete in 5-7 minutes)
- **Clear language:** Avoid jargon, use plain language
- **Mix question types:** Multiple choice, rating scales, open-ended
- **Test first:** Pilot with 10 people, refine before full launch
- **Mobile-friendly:** 70%+ will complete on phone
- **Anonymous:** Unless identity needed for follow-up

**Sample Survey Structure:**

**Section 1: Screening (2-3 questions)**
- Have you used [service] in the last 12 months? (Yes/No)
- If no, skip to Section 4

**Section 2: Experience (5-7 questions)**
- How did you access the service? (Website, mobile app, phone, in-person)
- Did you complete the service in one session? (Yes/No)
- If no, how many sessions did it take?
- Rate your satisfaction (1-5 scale: Very Dissatisfied to Very Satisfied)
- What was the most frustrating part? (Open text)
- What worked well? (Open text)

**Section 3: Future Preferences (3-4 questions)**
- Would you use digital ID to access this service? (Yes/No/Maybe)
- What device do you prefer for government services? (Smartphone, Tablet, Laptop, Desktop, Phone, In-person)
- How important is 24/7 access? (1-5 scale: Not Important to Very Important)

**Section 4: Demographics (3-5 questions)**
- Age range (18-25, 26-35, 36-45, 46-55, 56-65, 66+)
- Gender (Male, Female, Non-binary, Prefer not to say)
- Highest education (High school, Some college, Bachelor's, Graduate, Prefer not to say)
- Disability status (Yes, No, Prefer not to say) - if yes, ask type
- Location (Urban, Suburban, Rural)

**Distribution Channels:**
- Email to existing service users
- Pop-up on current service website
- Social media ads (Facebook, Instagram - target demographics)
- SMS to mobile users
- In-person at service centers (tablets)

**Incentives:**
- Prize draw (10 x $100 gift cards)
- Or guaranteed small incentive ($5 gift card for completion)

---

### **3. Usability Testing**

**When to Use:** Test prototypes or live services, identify usability issues

**Sample Size:** 5-8 participants per test round (Nielsen Norman Group: 5 users find 85% of issues)

**Test Format:** Moderated, one-on-one, 60 minutes

**Setup:**
- Quiet room, laptop/phone with prototype
- Facilitator (asks questions, observes)
- Optional: Observer behind one-way glass or video feed
- Screen recording + audio recording

**Test Script:**

**1. Introduction (5 min)**
- Thank you, purpose, think-aloud protocol
- "We're testing the service, not you - no wrong answers"
- "Please say what you're thinking as you use it"

**2. Pre-Test Questions (5 min)**
- Have you used [service] before?
- What do you expect from this service?
- How would you normally do this task?

**3. Task Scenarios (40 min)**
- Give realistic scenarios, observe as user attempts
- **Example:** "You just started a new business. Use this website to register your business name."
- **Observe:**
  - Can they find where to start?
  - Do they understand labels and instructions?
  - Where do they hesitate or get stuck?
  - Do they complete the task successfully?
  - How long does it take?

- **Probing Questions (during task):**
  - "What are you looking for?"
  - "What do you think this means?"
  - "What would you do next?"
  - "Is this what you expected?"

**4. Post-Test Questions (10 min)**
- How was that experience?
- Rate difficulty (1-5: Very Difficult to Very Easy)
- What was most confusing?
- What would you change?
- Would you use this service? (Yes/No, why?)

**Analysis:**
- Note every issue (categorize: critical, major, minor)
- Calculate success rate (% who completed task)
- Calculate average time to complete
- Prioritize fixes (impact x frequency)
- Retest after fixes in next iteration

---

### **4. Field Observation (Contextual Inquiry)**

**When to Use:** Understand real-world context, uncover unstated needs

**Method:** Observe users in their natural environment (home, office, service center)

**Duration:** 1-2 hours per observation

**Approach:**
- Shadow user as they complete task (e.g., filling out form, calling help desk)
- Ask questions while observing: "Why did you do that?" "What are you thinking?"
- Note environment (interruptions, tools used, workarounds)
- Take photos (with permission)

**Insights:**
- Real-world constraints (poor lighting, slow internet, multitasking)
- Workarounds users have developed
- Help-seeking behavior (who do they ask? what resources do they use?)
- Emotional reactions in real-time

**Example:** Observing elderly citizen at home trying to access pension online
- Discovered: Small font size difficult to read, bifocals needed
- Discovered: Pop-up blocker prevented page load (thought service was down)
- Discovered: Asked grandson for help (learned service needs to be explainable to helper)

---

## Prototype Testing Procedures

### **Fidelity Levels**

**Low Fidelity (Paper Prototypes):**
- **When:** Early stage, testing concepts and flows
- **Tools:** Paper, markers, sticky notes
- **Pros:** Fast, cheap, easy to change
- **Cons:** Limited interactivity, doesn't feel "real"
- **Use Case:** Test information architecture, basic flow

**Medium Fidelity (Wireframes):**
- **When:** Mid-stage, testing layout and interaction patterns
- **Tools:** Figma, Sketch, Adobe XD (clickable prototypes)
- **Pros:** Looks more real, can test interactions, shareable
- **Cons:** Still not final design, development effort to make changes
- **Use Case:** Test task flows, page layouts, navigation

**High Fidelity (Functional Prototypes):**
- **When:** Late stage, testing final design and performance
- **Tools:** HTML/CSS/JS or native mobile code
- **Pros:** Looks and feels like final product, can test performance
- **Cons:** Expensive to change, may feel too "done" for users to critique
- **Use Case:** Pilot launch, final validation before full build

**Rule of Thumb:** Start low fidelity, increase as confidence grows.

---

### **A/B Testing**

**When to Use:** Choose between two design options, optimize based on data

**Method:** Randomly show version A or B to users, measure which performs better

**Example:**
- **Version A:** "Apply for Benefits" button (green, top right)
- **Version B:** "Check Eligibility First" button (blue, centered)
- **Metric:** Click-through rate, completion rate, time to complete

**Sample Size:** Need statistical significance (typically 100+ users per version)

**Duration:** Run until 95% confidence in result (usually 1-2 weeks)

**Analysis:**
- Calculate conversion rate for each version
- Use statistical test (chi-square or t-test) to determine if difference is significant
- Declare winner, implement for all users

**Best Practices:**
- Test one variable at a time (button color OR text, not both)
- Have clear hypothesis: "We believe Version B will increase completion rate by 10%"
- Don't stop test early (wait for statistical significance)
- Learn from losers too (why didn't it work?)

---

## Life-Event Journey Mapping

### **What is Life-Event Design?**

**Traditional Approach:** Services organized by government structure (departments, programs)
- Problem: Citizens don't think in terms of departments
- Example: "Where do I register a birth?" (Health? Civil Registry? Local government?)

**Life-Event Approach:** Services organized by citizen life events
- Solution: Start with life events, design seamless journeys
- Example: "Having a Baby" (includes birth registration, health checks, parental leave, child benefits - all bundled)

---

### **Priority Life Events (Common Across Countries)**

**1. Birth & Early Childhood**
- Birth registration
- Newborn health screening
- Parental leave application
- Child benefits enrollment
- Childcare subsidies
- Immunization scheduling

**2. Education**
- School enrollment
- Scholarships and financial aid
- Student loans
- Credential verification (diplomas, transcripts)

**3. Employment**
- Job seeking (employment services)
- Unemployment benefits
- Work permits (for foreigners)
- Professional licensing
- Retirement planning

**4. Starting a Business**
- Business registration
- Tax ID application
- Licenses and permits
- Grant and loan applications
- Regulatory compliance

**5. Buying/Selling Property**
- Title search
- Property registration
- Mortgage assistance
- Moving (address change across agencies)

**6. Getting Married/Divorced**
- Marriage registration
- Name change
- Spousal benefits
- Divorce proceedings
- Child custody and support

**7. Health Crisis**
- Medical records access
- Insurance claims
- Disability benefits
- Long-term care
- Mental health services

**8. Losing a Job**
- Unemployment benefits
- Job training programs
- Career counseling
- Health insurance coverage
- Mortgage/loan assistance

**9. Retirement**
- Pension application
- Social security benefits
- Senior citizen benefits (discounts, healthcare)
- Estate planning

**10. Death of Loved One**
- Death registration
- Estate settlement
- Survivor benefits
- Bereavement support

---

### **Journey Mapping Process**

**Example: "Having a Baby" Journey**

**Step 1: Research**
- Interview 20+ parents who recently had babies
- Survey 200+ parents
- Observe at hospitals, clinics, government service centers
- Map what they currently do (all touchpoints with government and other services)

**Step 2: Map Current State (As-Is Journey)**

```
STAGE 1: Prenatal
- Week 8: Notify employer (parental leave forms)
- Week 12: Register for prenatal classes (health dept website)
- Week 20: Apply for maternity benefits (social security office, in-person, 2-hour wait)
- Week 30: Research childcare options (multiple websites, confusing eligibility)

STAGE 2: Birth
- Day 0: Baby born at hospital
- Day 1: Hospital submits birth notification to health dept (paper form)
- Day 3: Parent visits civil registry to register birth (30 min drive, 1-hour wait)
- Day 5: Apply for birth certificate (separate process, fee, 2-week wait)

STAGE 3: First Year
- Week 1: Schedule newborn health check (call clinic, limited hours)
- Week 2: Apply for child benefits (online, but form asks for birth cert number - don't have yet)
- Week 4: Receive birth certificate (by mail)
- Week 6: Re-apply for child benefits (now have cert number)
- Month 2: Immunization (multiple appointments, manual record keeping)
- Month 6: Apply for childcare subsidy (different website, different login)
- Month 12: Update tax return to include dependent (manual entry of child details)
```

**Pain Points Identified:**
- 7 different government touchpoints, 5 different websites/offices
- Duplicate data entry (baby's name, DOB, parent info entered 7+ times)
- Waiting for birth certificate delays other applications
- In-person visits difficult with newborn
- Fragmented information (no single source of truth)
- Emotional stress (parents overwhelmed, sleep-deprived)

**Step 3: Design Future State (To-Be Journey)**

**Vision:** "One notification, everything done"

```
STAGE 1: Prenatal
- Week 8: Log into government portal (using digital ID)
- Week 8: Click "I'm Pregnant" - triggers life event workflow
- Week 8: System pre-fills forms with known data, asks for missing info (due date, hospital)
- Week 8: System suggests available services (parental leave, benefits, classes) - apply for all in one session
- Week 8: Confirmation and calendar (all appointments and deadlines)

STAGE 2: Birth
- Day 0: Baby born, hospital submits birth notification electronically (via API)
- Day 1: Parent receives notification on phone: "Congratulations! Next steps available."
- Day 1: Parent logs in, confirms baby details (name, time), approves birth registration
- Day 1: System auto-generates birth certificate (digital + mailed paper)
- Day 1: System auto-enrolls in child benefits (if eligible, based on income data)
- Day 1: System schedules newborn health check (user confirms preferred time)

STAGE 3: First Year
- Proactive notifications: "Immunization due next week - book now" (one click)
- Auto-updates: Tax record updated with dependent (no action needed)
- Childcare support: System surfaces subsidy application when parent searches childcare
- Unified health record: All immunizations logged automatically (via clinic API)
```

**Benefits:**
- 1 government touchpoint (portal/app) instead of 7
- Data entered once, reused (via APIs and "Tell Us Once")
- Birth certificate instant (digital) + mailed within 3 days
- Proactive nudges (notifications for upcoming needs)
- Reduced stress (parents can focus on baby, not paperwork)

**Estimated Impact:**
- Time saved: 10-15 hours per parent (vs. current state)
- Satisfaction: 4.5+/5.0 (vs. current 2.8/5.0)
- Benefit take-up: 95%+ (vs. current 60% - many don't know they're eligible)

---

## Persona Development

### **Persona Template**

**Name:** [Fictional name, relatable]
**Age:** [Range]
**Photo:** [Stock photo or illustration]
**Quote:** [Memorable statement that captures essence]

**Demographics:**
- Occupation
- Income level
- Education
- Family status
- Location (urban/rural)

**Psychographics:**
- Tech-savviness (low/medium/high)
- Government trust (low/medium/high)
- Time availability (busy/moderate/flexible)
- Language(s)
- Disabilities/accessibility needs

**Goals:**
- What do they want to achieve? (3-5 goals)

**Frustrations:**
- What blocks them today? (3-5 frustrations)

**Behaviors:**
- How do they currently solve problems?
- What devices do they use?
- When do they interact with government?
- Who do they ask for help?

**Service Usage:**
- What government services have they used?
- Preferred channel (online, phone, in-person)?
- Frequency of interaction

---

### **Example Persona: "Ahmed" (Tech-Savvy Professional)**

**Name:** Ahmed Hassan
**Age:** 29
**Occupation:** Software Engineer
**Quote:** "If my bank can do it on an app, why can't the government?"

**Demographics:**
- Income: Upper-middle class
- Education: Bachelor's in Computer Science
- Family: Single, no children
- Location: Urban (capital city)

**Tech Comfort: High**
- Uses smartphone for everything (banking, shopping, health tracking)
- Expects seamless digital experiences
- Rarely carries physical documents

**Goals:**
- Renew driver's license online (no DMV visit)
- Pay taxes easily (mobile app)
- Access government services 24/7

**Frustrations:**
- Government websites outdated, clunky
- Can't use digital ID for everything yet
- Has to print and scan documents (why?!)
- Operating hours (9-5) conflict with work

**Behaviors:**
- Mobile-first (barely uses laptop for personal tasks)
- Expects instant results
- Abandons if process takes >5 minutes
- Reads online reviews before trying new service

**Design Implications:**
- Mobile app is must-have (not just responsive website)
- Digital ID integration critical
- Speed and simplicity paramount
- Modern UI (matches expectations from private sector apps)

---

### **Example Persona: "Grace" (Elderly, Low Tech Literacy)**

**Name:** Grace Wong
**Age:** 72
**Occupation:** Retired Teacher
**Quote:** "I just want to talk to a real person who can help me."

**Demographics:**
- Income: Fixed pension
- Education: Bachelor's in Education
- Family: Widow, 2 adult children (live in different city)
- Location: Suburban

**Tech Comfort: Low**
- Has smartphone (gift from children) but only uses for calls and WhatsApp
- Prefers in-person or phone interactions
- Intimidated by complex websites

**Goals:**
- Access pension and healthcare benefits
- Renew senior citizen ID
- Get help understanding government letters (confusing jargon)

**Frustrations:**
- Websites too complicated, small fonts
- Afraid of making mistakes online (security concerns)
- Long wait times at service centers (hard to stand)
- No family nearby to help

**Behaviors:**
- Visits service centers in person (despite difficulty)
- Asks children for help (but doesn't want to burden them)
- Takes handwritten notes (brings notebook to appointments)
- Trusts government but overwhelmed by processes

**Accessibility Needs:**
- Bifocals (needs large, clear text)
- Arthritis (difficulty with small buttons, touch targets)
- Prefers high contrast (easier to read)

**Design Implications:**
- Maintain in-person and phone channels (digital alone excludes)
- Assisted digital services (staff who can help)
- Large fonts, high contrast, simple language
- Video tutorials (daughter can help her watch)
- Confirmation SMS (reassurance that action completed)

---

## Accessibility Testing (WCAG 2.1 AA)

### **WCAG 2.1 AA Requirements (Key Criteria)**

**1. Perceivable**
- **Text Alternatives:** Alt text for all images
- **Captions:** Captions for videos
- **Adaptable:** Content can be presented in different ways (responsive design)
- **Distinguishable:** Text contrast ratio min 4.5:1 (normal text), 3:1 (large text)

**2. Operable**
- **Keyboard Accessible:** All functionality available via keyboard (no mouse required)
- **Enough Time:** Users can extend time limits (or no time limits for critical tasks)
- **Seizures:** No flashing content >3 times per second
- **Navigable:** Clear navigation, skip links, descriptive headings

**3. Understandable**
- **Readable:** Language specified, plain language used
- **Predictable:** Consistent navigation, no unexpected changes
- **Input Assistance:** Error messages clear, suggestions provided

**4. Robust**
- **Compatible:** Works with assistive technologies (screen readers, voice control)

---

### **Accessibility Testing Checklist**

**Automated Testing (Tools):**
- **WAVE (WebAIM):** Browser extension, identifies accessibility errors
- **Axe DevTools:** Browser extension, WCAG compliance checking
- **Lighthouse (Chrome):** Built-in audit tool, includes accessibility score

**Manual Testing:**

**1. Keyboard Navigation**
- [ ] Can you navigate entire site using only Tab, Shift+Tab, Enter, Spacebar?
- [ ] Is focus indicator visible (outline around active element)?
- [ ] Logical tab order (left-to-right, top-to-bottom)?
- [ ] No keyboard traps (can navigate back out of modals, menus)?

**2. Screen Reader Testing**
- [ ] Test with NVDA (Windows) or VoiceOver (Mac/iOS)
- [ ] All images have alt text (or marked decorative if no meaning)?
- [ ] Headings structured correctly (H1 → H2 → H3, not skipping levels)?
- [ ] Form labels associated with inputs (screen reader announces label)?
- [ ] Links descriptive ("Download tax form" not "Click here")?

**3. Visual Testing**
- [ ] Text contrast sufficient (use Contrast Checker tool)?
- [ ] Text resizable to 200% without loss of functionality?
- [ ] Color not used as only indicator (e.g., red for error + icon + text)?
- [ ] Works at 1280x1024 and 320x256 (zoom and mobile)?

**4. Forms & Errors**
- [ ] Error messages clear ("Email address missing" not "Error in field 3")?
- [ ] Error linked to field (screen reader can navigate to error source)?
- [ ] Suggestions provided ("Format: MM/DD/YYYY")?
- [ ] Success confirmation (after form submission)?

**5. Multimedia**
- [ ] Videos have captions?
- [ ] Audio descriptions for video content (if visual info critical)?
- [ ] Transcripts provided for audio content?

**6. Assistive Technology Compatibility**
- [ ] Test with screen reader (NVDA, JAWS, VoiceOver)
- [ ] Test with voice control (Dragon NaturallySpeaking, Voice Control on iOS)
- [ ] Test with screen magnification (ZoomText, built-in OS zoom)

---

### **User Testing with People with Disabilities**

**Recruit Diverse Participants:**
- Visual impairments (blind, low vision, color blind)
- Hearing impairments (deaf, hard of hearing)
- Motor impairments (limited dexterity, use of assistive devices)
- Cognitive impairments (dyslexia, ADHD, autism)

**Sample Size:** 5+ participants with disabilities per round

**Adaptations:**
- Longer session time (90 min instead of 60)
- Provide breaks
- Flexible format (remote testing may be easier)
- Compensate fairly (higher incentive for specialized feedback)

**Observe:**
- Can they complete tasks independently?
- What assistive technology do they use (screen reader, switch control, etc.)?
- Where do they struggle?
- What workarounds have they developed?

**Learn & Fix:**
- Prioritize accessibility fixes (not "nice to have" - legal requirement)
- Retest after fixes
- Involve people with disabilities throughout design (not just testing at end)

---

## Engagement Metrics

### **Research Metrics**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Citizens Engaged (annually)** | 1,000+ | Research participants, workshop attendees, survey respondents |
| **Diversity of Participants** | Matches national demographics | Age, gender, disability, income, education, geography |
| **Research Studies Conducted** | 10+ per major service | Interviews, surveys, usability tests per service |
| **Personas Developed** | 20+ (across all services) | Documented and actively used in design |
| **Journey Maps Created** | 10+ life events | Mapped current and future state |

---

### **Design Quality Metrics**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Service Standard Compliance** | 100% (before launch) | Independent assessment against 16 criteria |
| **Accessibility Compliance (WCAG 2.1 AA)** | 100% | Automated + manual testing |
| **Usability Test Success Rate** | 80%+ | % of users who complete task successfully |
| **Time to Complete Task** | 50%+ reduction (vs. old service) | Usability testing measurement |
| **Prototype Iterations** | 3+ rounds per service | Track versions tested |

---

### **User Satisfaction Metrics**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Citizen Satisfaction** | 4.5+/5.0 | Post-service survey (all users) |
| **Net Promoter Score (NPS)** | 60+ | "How likely are you to recommend this service?" (0-10 scale) |
| **First-Time Completion Rate** | 80%+ | % who complete service in one session without help |
| **Support Ticket Volume** | 40%+ reduction | Compare to old service |
| **Accessibility Satisfaction** | 4.0+/5.0 | Survey of users with disabilities |

---

### **Process Metrics**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Avg Time from Research to Launch** | 4-6 months | Track per service |
| **Redesign Rate** | <10% | % of services requiring major redesign post-launch |
| **User Research Hours (per service)** | 80-120 hours | Track researcher time |
| **Co-Design Workshops (per service)** | 3+ | Kickoff, ideation, testing |

---

## Case Studies & Templates

### **Case Study: UK Gov.uk**

**Context:**
- Consolidated 300+ government websites into one (www.gov.uk)
- User needs-driven design
- Plain language, mobile-first
- Continuous iteration based on data

**Approach:**
- Created Government Digital Service (GDS) - central team
- Established Service Standard (16 criteria)
- Embedded user researchers in every project team
- Published design patterns and style guide (open source)

**Research Methods Used:**
- 1,000+ hours of user research (interviews, usability testing) before launch
- Ongoing research: 20+ researchers, continuous testing
- Data-driven: A/B testing, analytics, user feedback

**Results:**
- User satisfaction: 80%+ (vs. 40% for old websites)
- Task completion rate: 85%+ (vs. 60%)
- Maintenance costs reduced by 50% (simpler design, less content)
- Awards: D&AD Black Pencil (highest design honor)

**Lessons:**
- Central team + design system = consistency at scale
- Service Standard as guardrail (prevents bad design from launching)
- "Start with user needs, not government structure" - cardinal rule
- Data + research beats opinions (make decisions based on evidence)

---

### **Case Study: Denmark Life Event Portal**

**Context:**
- Designed services around life events, not departments
- Example: "Having a Baby" bundles 12 services from 5 agencies

**Approach:**
- Mapped 20 major life events (from research with 500+ citizens)
- Redesigned services around these events
- "Tell Us Once" - share data across agencies
- Proactive notifications (reminders for upcoming needs)

**Example: Moving to New Address**
- Old way: Notify 8+ agencies separately (DMV, tax office, health, postal, etc.)
- New way: Update address once, system propagates to all agencies
- Result: 15 minutes vs. 3+ hours

**Results:**
- Digital service usage: 95%+ (vs. 60% pre-redesign)
- Citizen satisfaction: 4.6/5.0
- Time savings: 50M+ hours annually (across all life events)

**Lessons:**
- Life events resonate with citizens (intuitive mental model)
- Data sharing critical (APIs between agencies)
- Proactive > Reactive (nudge citizens before deadlines, don't wait for them to find service)

---

### **Template: Research Consent Form**

**Government of [Country Name]**
**User Research Participation Consent Form**

**Project Name:** [Service Name] User Research

**Purpose:** We are designing a new [service description]. Your feedback will help us make it easy to use and accessible to everyone.

**What You'll Do:**
- Participate in [interview/usability test/workshop]
- Duration: [60 minutes]
- Activities: [Answer questions / try out prototype / share ideas]

**Your Rights:**
- **Voluntary:** You can skip questions or stop anytime
- **Confidential:** Your name won't be shared publicly
- **Data Use:** We'll use your feedback to improve the service, may quote anonymously in reports
- **Recording:** [Audio/Video] recording for our reference only (not published)
- **Compensation:** [Gift card amount] for your time

**Contact:**
- Questions? Email: [researcher@gov.xx]
- Complaints? Privacy Officer: [privacy@gov.xx]

**Consent:**
- [ ] I agree to participate
- [ ] I agree to be recorded (audio/video)
- [ ] I agree to be quoted anonymously

**Signature:** __________________ **Date:** __________

---

## Deliverables by PPT Framework

### People Deliverables (50% of effort)

**Team Building:**
- Co-Design Center of Excellence established (10-15 FTEs)
- User researchers with government domain expertise
- Service designers and interaction designers
- Content designers and accessibility specialists
- Facilitation and community engagement experts

**Citizen Engagement:**
- 1,000+ citizens engaged in co-design activities within 12 months
- Representative panels across demographics (age, disability, geography, literacy)
- Citizen design partners program (ongoing advisory group)
- Community outreach partnerships
- Compensation and incentive framework for participants

**Agency Capacity Building:**
- 100+ government staff trained in user research methods
- Service design workshops for agency teams
- Accessibility champions network (30+ agencies)
- Design thinking training for executives
- User research mentorship program

**Culture Change:**
- User-centered design advocacy and communications
- Success stories and case studies showcasing impact
- Service design community of practice
- Monthly show-and-tells and learning sessions
- Design awards recognizing excellent citizen-centered services

### Process Deliverables (40% of effort)

**Co-Design Methodology:**
- Service Standard (16 criteria adapted from UK GDS)
- Design phase playbooks (discovery, alpha, beta, live)
- User research protocols and ethics guidelines
- Recruitment and screening procedures
- Facilitation guides for workshops and testing

**Design Standards & Patterns:**
- Government Design System (components, patterns, styles)
- Content style guide (plain language, readability)
- Accessibility guidelines (WCAG 2.1 AA compliance)
- Usability heuristics and best practices
- Mobile-first responsive design standards

**Quality Assurance:**
- Service assessment process and criteria
- Peer review framework
- User testing protocols (moderated and unmoderated)
- Accessibility audit checklist
- Analytics review standards (defining success metrics)

**Continuous Improvement:**
- User feedback collection mechanisms
- Regular usability testing schedules
- Service performance reviews (quarterly)
- Iteration planning based on data
- Knowledge sharing and documentation practices

### Technology Deliverables (10% of effort)

**Research & Testing Tools:**
- User research repository and analysis platform
- Remote usability testing software
- Survey and feedback collection tools
- Screen recording and session replay
- Accessibility testing automation

**Prototyping & Collaboration:**
- Design collaboration platform (Figma, Sketch, or similar)
- Prototyping tools for quick iteration
- Version control for design assets
- Stakeholder collaboration portals
- Design handoff tools for developers

**Analytics & Measurement:**
- Digital analytics platform (user behavior tracking)
- Service performance dashboards
- A/B testing infrastructure
- Heatmaps and click tracking
- User satisfaction measurement tools (surveys, NPS)

**Design System Infrastructure:**
- Component library (HTML/CSS/JavaScript)
- Pattern library with code examples
- Design token system
- Automated visual regression testing
- Documentation and showcase website

---

## Conclusion

Co-design and citizen engagement are not optional in the GaaS framework - they are foundational. Services designed without user input fail more often, cost more to fix, and frustrate citizens.

**Critical Success Factors:**
- **Executive Sponsorship:** Leadership commitment to user-centered design
- **Dedicated Team:** User researchers, designers, content specialists (not just developers)
- **Systematic Process:** Embed co-design in every project (not ad-hoc)
- **Diverse Participation:** Engage underrepresented groups (elderly, disabled, rural, minorities)
- **Iterate Based on Feedback:** Launch early, improve continuously
- **Accessibility Non-Negotiable:** WCAG 2.1 AA minimum (legal and moral imperative)

**Next Steps:**
1. Establish Co-Design Center of Excellence (CoE)
2. Train agency staff in user research methods
3. Select 2-3 pilot services for intensive co-design
4. Develop design system (patterns, components, guidelines)
5. Mandate Service Standard compliance for all major services
6. Publish case studies and learnings (build momentum)

**Remember:** Co-design is not just about better services - it's about rebuilding trust between government and citizens. When citizens see their feedback implemented, they feel heard and valued. That's the foundation of a responsive, modern government.

---

**Document Version:** 1.0
**Last Updated:** 2025-10-18
**Owner:** GaaS Implementation Team
**Status:** Final for Review

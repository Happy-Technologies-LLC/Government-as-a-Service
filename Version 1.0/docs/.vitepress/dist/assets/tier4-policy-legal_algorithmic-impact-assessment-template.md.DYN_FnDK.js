import{_ as n,c as s,o as l,ag as a,j as e,a as i}from"./chunks/framework.UpazuZ_L.js";const g=JSON.parse('{"title":"Algorithmic Impact Assessment (AIA) Template","description":"","frontmatter":{},"headers":[],"relativePath":"tier4-policy-legal/algorithmic-impact-assessment-template.md","filePath":"tier4-policy-legal/algorithmic-impact-assessment-template.md"}'),r={name:"tier4-policy-legal/algorithmic-impact-assessment-template.md"};function o(_,t,d,c,p,u){return l(),s("div",null,[...t[0]||(t[0]=[a(`<h1 id="algorithmic-impact-assessment-aia-template" tabindex="-1">Algorithmic Impact Assessment (AIA) Template <a class="header-anchor" href="#algorithmic-impact-assessment-aia-template" aria-label="Permalink to &quot;Algorithmic Impact Assessment (AIA) Template&quot;">​</a></h1><h2 id="government-as-a-service-gaas-tier-4-policy-template" tabindex="-1">Government as a Service (GaaS) - Tier 4 Policy Template <a class="header-anchor" href="#government-as-a-service-gaas-tier-4-policy-template" aria-label="Permalink to &quot;Government as a Service (GaaS) - Tier 4 Policy Template&quot;">​</a></h2><p><strong>Version:</strong> 1.0 <strong>Status:</strong> Template for National/Agency Adaptation <strong>Last Updated:</strong> October 2025 <strong>Template Classification:</strong> Public</p><hr><h2 id="executive-summary" tabindex="-1">Executive Summary <a class="header-anchor" href="#executive-summary" aria-label="Permalink to &quot;Executive Summary&quot;">​</a></h2><p>This Algorithmic Impact Assessment (AIA) template provides a comprehensive framework for evaluating the risks, benefits, and impacts of artificial intelligence (AI) and automated decision-making systems in government services. It ensures that algorithmic systems are fair, transparent, accountable, and aligned with public values.</p><p><strong>Purpose:</strong></p><ul><li>Identify and mitigate algorithmic bias and discrimination</li><li>Ensure transparency and explainability of automated decisions</li><li>Protect individual rights and promote fairness</li><li>Build public trust in AI-enabled government services</li><li>Enable responsible innovation with appropriate safeguards</li></ul><p><strong>When to Use:</strong></p><ul><li>Deploying AI/ML systems for decision-making</li><li>Implementing automated screening or scoring systems</li><li>Using predictive analytics affecting individuals</li><li>Applying algorithmic risk assessments</li><li>Utilizing facial recognition or biometric systems</li></ul><hr><h2 id="part-1-aia-overview-and-methodology" tabindex="-1">Part 1: AIA Overview and Methodology <a class="header-anchor" href="#part-1-aia-overview-and-methodology" aria-label="Permalink to &quot;Part 1: AIA Overview and Methodology&quot;">​</a></h2><h3 id="_1-1-what-is-an-algorithmic-impact-assessment" tabindex="-1">1.1 What is an Algorithmic Impact Assessment? <a class="header-anchor" href="#_1-1-what-is-an-algorithmic-impact-assessment" aria-label="Permalink to &quot;1.1 What is an Algorithmic Impact Assessment?&quot;">​</a></h3><p>An Algorithmic Impact Assessment (AIA) is a structured process to evaluate the potential impacts of automated decision-making systems before deployment and throughout their operational lifecycle.</p><p><strong>Key Objectives:</strong></p><ol><li><strong>Identify potential harms</strong> before they occur</li><li><strong>Assess fairness and bias</strong> across different population groups</li><li><strong>Ensure transparency</strong> in how decisions are made</li><li><strong>Enable accountability</strong> through clear governance</li><li><strong>Protect individual rights</strong> including appeal and redress</li><li><strong>Promote public trust</strong> through responsible AI practices</li></ol><h3 id="_1-2-when-aia-is-required" tabindex="-1">1.2 When AIA is Required <a class="header-anchor" href="#_1-2-when-aia-is-required" aria-label="Permalink to &quot;1.2 When AIA is Required&quot;">​</a></h3><p><strong>Mandatory AIA Required For:</strong></p><ul><li>[ ] Automated decisions directly affecting individual rights, benefits, or services</li><li>[ ] Use of AI/ML for eligibility determinations (benefits, licenses, permits)</li><li>[ ] Predictive risk scoring or profiling systems</li><li>[ ] Facial recognition or biometric identification</li><li>[ ] Automated content moderation or filtering</li><li>[ ] Algorithmic allocation of resources or opportunities</li><li>[ ] Fraud detection systems affecting individuals</li><li>[ ] Any system with potential for significant impact on vulnerable populations</li></ul><p><strong>Recommended AIA (Best Practice):</strong></p><ul><li>[ ] Recommendation systems influencing individual choices</li><li>[ ] Chatbots and virtual assistants providing guidance</li><li>[ ] Optimization algorithms affecting service delivery</li><li>[ ] Data analytics for policy development</li><li>[ ] Automation of administrative processes</li></ul><p><strong>AIA Not Required:</strong></p><ul><li>[ ] Simple rule-based automation with no learning component</li><li>[ ] Fully manual decision-making with no algorithmic input</li><li>[ ] Anonymous aggregate analytics with no individual impact</li></ul><p><em>Note: Consult with your Ethics Board or Data Protection Officer if uncertain.</em></p><h3 id="_1-3-relationship-to-privacy-impact-assessment" tabindex="-1">1.3 Relationship to Privacy Impact Assessment <a class="header-anchor" href="#_1-3-relationship-to-privacy-impact-assessment" aria-label="Permalink to &quot;1.3 Relationship to Privacy Impact Assessment&quot;">​</a></h3><p><strong>Integration with PIA:</strong></p><ul><li>Many AI systems also process personal data and require a Privacy Impact Assessment (PIA)</li><li>This AIA should be conducted in conjunction with PIA when applicable</li><li>Some sections may reference or rely on PIA findings</li><li>Both assessments should be reviewed by Data Protection Officer</li></ul><p><strong>Unique Focus of AIA:</strong></p><ul><li>Algorithmic fairness and bias (beyond privacy)</li><li>Explainability and transparency of decisions</li><li>Accuracy and reliability of predictions</li><li>Human oversight and appeal mechanisms</li><li>Broader societal and ethical implications</li></ul><h3 id="_1-4-aia-process-overview" tabindex="-1">1.4 AIA Process Overview <a class="header-anchor" href="#_1-4-aia-process-overview" aria-label="Permalink to &quot;1.4 AIA Process Overview&quot;">​</a></h3><p><strong>Phase 1: Scoping and Planning (2-3 weeks)</strong></p><ul><li>Define system purpose and scope</li><li>Identify stakeholders and impact areas</li><li>Determine assessment approach</li><li>Establish assessment team</li></ul><p><strong>Phase 2: System Documentation (3-4 weeks)</strong></p><ul><li>Document algorithmic methodology</li><li>Map data inputs and decision outputs</li><li>Identify training data sources and characteristics</li><li>Describe model architecture and parameters</li></ul><p><strong>Phase 3: Fairness and Bias Assessment (4-6 weeks)</strong></p><ul><li>Define fairness criteria for context</li><li>Test for bias across demographic groups</li><li>Evaluate disparate impact</li><li>Assess representation in training data</li></ul><p><strong>Phase 4: Performance and Accuracy Evaluation (3-4 weeks)</strong></p><ul><li>Measure prediction accuracy</li><li>Identify error patterns and edge cases</li><li>Assess reliability and robustness</li><li>Evaluate performance across subgroups</li></ul><p><strong>Phase 5: Transparency and Explainability Review (2-3 weeks)</strong></p><ul><li>Document how system makes decisions</li><li>Develop user-facing explanations</li><li>Create appeal and review processes</li><li>Design transparency reporting</li></ul><p><strong>Phase 6: Impact Analysis (3-4 weeks)</strong></p><ul><li>Assess impacts on individuals and communities</li><li>Evaluate effects on vulnerable populations</li><li>Consider broader societal implications</li><li>Engage affected stakeholders</li></ul><p><strong>Phase 7: Mitigation and Governance (3-4 weeks)</strong></p><ul><li>Design bias mitigation strategies</li><li>Establish ongoing monitoring procedures</li><li>Create human oversight mechanisms</li><li>Develop incident response protocols</li></ul><p><strong>Phase 8: Review, Approval, and Publication (2-3 weeks)</strong></p><ul><li>Expert review (technical, ethical, legal)</li><li>Senior management approval</li><li>Public transparency reporting</li><li>Ongoing monitoring plan</li></ul><h3 id="_1-5-roles-and-responsibilities" tabindex="-1">1.5 Roles and Responsibilities <a class="header-anchor" href="#_1-5-roles-and-responsibilities" aria-label="Permalink to &quot;1.5 Roles and Responsibilities&quot;">​</a></h3><p><strong>Algorithmic Accountability Lead</strong></p><ul><li>Owns the AIA process</li><li>Coordinates assessment activities</li><li>Reports to senior management</li><li>Ensures compliance with requirements</li></ul><p><strong>Data Scientist / AI Developer</strong></p><ul><li>Documents technical details</li><li>Conducts bias and performance testing</li><li>Implements mitigation measures</li><li>Provides ongoing technical monitoring</li></ul><p><strong>Subject Matter Expert</strong></p><ul><li>Provides domain knowledge</li><li>Identifies use cases and edge cases</li><li>Reviews decision logic for appropriateness</li><li>Validates outputs</li></ul><p><strong>Ethics Advisor / Ethicist</strong></p><ul><li>Reviews ethical implications</li><li>Advises on fairness criteria</li><li>Identifies potential harms</li><li>Recommends safeguards</li></ul><p><strong>Data Protection Officer</strong></p><ul><li>Reviews privacy implications</li><li>Ensures GDPR/privacy law compliance</li><li>Advises on individual rights</li><li>Approves data protection measures</li></ul><p><strong>Legal Counsel</strong></p><ul><li>Reviews legal compliance</li><li>Assesses liability and risk</li><li>Advises on regulatory requirements</li><li>Reviews terms and conditions</li></ul><p><strong>Community Representative</strong></p><ul><li>Represents affected populations</li><li>Provides lived experience perspective</li><li>Identifies community concerns</li><li>Validates impact assessments</li></ul><hr><h2 id="part-2-system-description-and-documentation" tabindex="-1">Part 2: System Description and Documentation <a class="header-anchor" href="#part-2-system-description-and-documentation" aria-label="Permalink to &quot;Part 2: System Description and Documentation&quot;">​</a></h2><h3 id="section-a-system-overview" tabindex="-1">Section A: System Overview <a class="header-anchor" href="#section-a-system-overview" aria-label="Permalink to &quot;Section A: System Overview&quot;">​</a></h3><p><strong>A1. System Identification</strong></p><ul><li>System Name: _____________________________</li><li>System ID/Reference: _____________________________</li><li>AIA Reference Number: _____________________________</li><li>Assessment Date: _____________________________</li><li>Version: _____________________________</li></ul><p><strong>A2. System Purpose and Objectives</strong></p><p><em>Describe what the system does and why it exists (300-500 words):</em></p><ul><li>What problem does it solve?</li><li>What decisions or recommendations does it make?</li><li>Who benefits from the system?</li><li>What are the expected outcomes?</li></ul><p>[Your response here]</p><p><strong>A3. System Owner and Team</strong></p><ul><li>Owning Agency: _____________________________</li><li>System Owner (Name, Title): _____________________________</li><li>Technical Lead: _____________________________</li><li>Algorithmic Accountability Lead: _____________________________</li><li>Data Protection Officer: _____________________________</li><li>Ethics Advisor: _____________________________</li></ul><p><strong>A4. System Status</strong></p><ul><li>[ ] Concept / Planning</li><li>[ ] Development / Training</li><li>[ ] Testing / Validation</li><li>[ ] Pilot / Limited Deployment</li><li>[ ] Full Production</li><li>[ ] Update to Existing System</li></ul><p><strong>A5. Deployment Context</strong></p><p><em>Where and how will the system be used?</em></p><ul><li>Geographic Scope: _____________________________</li><li>Organizational Units: _____________________________</li><li>User Roles: _____________________________</li><li>Integration with Other Systems: _____________________________</li><li>Anticipated Volume: _____ decisions/predictions per [day/month/year]</li></ul><hr><h3 id="section-b-algorithmic-methodology" tabindex="-1">Section B: Algorithmic Methodology <a class="header-anchor" href="#section-b-algorithmic-methodology" aria-label="Permalink to &quot;Section B: Algorithmic Methodology&quot;">​</a></h3><p><strong>B1. Type of Algorithmic System</strong></p><p><em>Primary classification:</em></p><ul><li>[ ] Rule-based system (if-then logic, decision trees)</li><li>[ ] Machine learning - Supervised learning (trained on labeled data)</li><li>[ ] Machine learning - Unsupervised learning (clustering, pattern detection)</li><li>[ ] Machine learning - Reinforcement learning (learns through trial and error)</li><li>[ ] Deep learning / neural networks</li><li>[ ] Natural language processing</li><li>[ ] Computer vision</li><li>[ ] Ensemble methods (combining multiple models)</li><li>[ ] Hybrid (combination of approaches)</li></ul><p><em>Specific algorithm(s) or model(s) used:</em> [e.g., Logistic regression, Random forest, Gradient boosting, Neural network architecture]</p><p><strong>B2. Decision-Making Role</strong></p><p><em>What role does the algorithm play?</em></p><ul><li>[ ] <strong>Fully automated decision</strong> - No human involvement</li><li>[ ] <strong>Human-in-the-loop</strong> - Human reviews and approves algorithmic recommendations</li><li>[ ] <strong>Human-on-the-loop</strong> - Human oversight with ability to intervene</li><li>[ ] <strong>Decision support</strong> - Provides information to inform human decisions</li><li>[ ] <strong>Prioritization or triage</strong> - Ranks or sorts for human review</li></ul><p><em>If automated, what is the justification for automation?</em> [Efficiency, consistency, scale, speed, etc.]</p><p><em>Can individuals request human review?</em></p><ul><li>[ ] Yes - Process: _____________________________</li><li>[ ] No - Justification: _____________________________</li></ul><p><strong>B3. Input Data</strong></p><p><em>What data is used to make decisions?</em></p><table tabindex="0"><thead><tr><th>Data Category</th><th>Specific Variables</th><th>Source</th><th>Quality/Accuracy</th><th>Update Frequency</th></tr></thead><tbody><tr><td>Demographics</td><td>Age, gender, location</td><td>Government registry</td><td>95% accurate</td><td>Real-time</td></tr><tr><td>[Add rows]</td><td></td><td></td><td></td><td></td></tr></tbody></table><p><strong>Data Sources:</strong></p><ul><li>[ ] Individual-provided (applications, forms)</li><li>[ ] Government databases</li><li>[ ] Third-party data providers</li><li>[ ] Public datasets</li><li>[ ] Social media or web scraping</li><li>[ ] Sensors or IoT devices</li><li>[ ] Other: _____________________________</li></ul><p><strong>Data Quality Assessment:</strong></p><ul><li>Completeness: _____ % of records have all required fields</li><li>Accuracy: _____ % of data validated as correct</li><li>Timeliness: Updated every _____ [hours/days/months]</li><li>Representativeness: Does data represent all affected populations? [ ] Yes [ ] No [ ] Unknown</li></ul><p><em>If data quality issues exist, describe:</em> [Your response here]</p><p><strong>B4. Training Data (for ML systems)</strong></p><p><em>What data was used to train the model?</em></p><ul><li>Training Dataset Size: _____ records</li><li>Training Period: From _____ to _____</li><li>Data Source: _____________________________</li><li>Geographic Coverage: _____________________________</li></ul><p><strong>Demographic Representation in Training Data:</strong></p><table tabindex="0"><thead><tr><th>Demographic Group</th><th>Percentage in Training Data</th><th>Percentage in Target Population</th><th>Representative?</th></tr></thead><tbody><tr><td>Gender - Female</td><td>52%</td><td>51%</td><td>Yes</td></tr><tr><td>Gender - Male</td><td>48%</td><td>49%</td><td>Yes</td></tr><tr><td>Age - Under 25</td><td>15%</td><td>22%</td><td>Under-represented</td></tr><tr><td>[Add rows for all relevant groups]</td><td></td><td></td><td></td></tr></tbody></table><p><em>If training data is not representative, what is the impact?</em> [Your response here]</p><p><strong>Historical Bias in Training Data:</strong></p><ul><li>Does training data reflect historical discrimination or bias? [ ] Yes [ ] No [ ] Unknown</li><li>If yes, describe: _____________________________</li><li>Mitigation measures: _____________________________</li></ul><p><strong>B5. Output and Decision Logic</strong></p><p><em>What does the system output?</em></p><ul><li>[ ] Binary classification (yes/no, approve/deny)</li><li>[ ] Multi-class classification (categories)</li><li>[ ] Risk score or probability (0-100, low/medium/high)</li><li>[ ] Ranking or prioritization</li><li>[ ] Prediction (numerical value)</li><li>[ ] Recommendation</li><li>[ ] Other: _____________________________</li></ul><p><em>How is the output used to make a decision?</em> [e.g., &quot;Scores above 70 are approved automatically, 50-69 flagged for human review, below 50 denied&quot;]</p><p><strong>Decision Thresholds:</strong></p><ul><li>Threshold Value(s): _____________________________</li><li>How was threshold determined: _____________________________</li><li>Who can modify threshold: _____________________________</li></ul><p><strong>B6. Model Performance Metrics</strong></p><p><em>For ML systems, report performance on test data:</em></p><table tabindex="0"><thead><tr><th>Metric</th><th>Overall</th><th>Group 1 [specify]</th><th>Group 2 [specify]</th><th>Acceptable?</th></tr></thead><tbody><tr><td>Accuracy</td><td>85%</td><td>87%</td><td>78%</td><td>Target &gt;80%</td></tr><tr><td>Precision (positive predictive value)</td><td>82%</td><td>85%</td><td>73%</td><td></td></tr><tr><td>Recall (sensitivity)</td><td>88%</td><td>90%</td><td>81%</td><td></td></tr><tr><td>False positive rate</td><td>12%</td><td>10%</td><td>19%</td><td></td></tr><tr><td>False negative rate</td><td>12%</td><td>10%</td><td>19%</td><td></td></tr><tr><td>F1 Score</td><td>85%</td><td>87.5%</td><td>77%</td><td></td></tr><tr><td>AUC-ROC</td><td>0.91</td><td>0.93</td><td>0.85</td><td></td></tr></tbody></table><p><em>Performance disparities between groups:</em> [Analyze whether differences are acceptable or indicate bias]</p><hr><h3 id="section-c-fairness-and-bias-assessment" tabindex="-1">Section C: Fairness and Bias Assessment <a class="header-anchor" href="#section-c-fairness-and-bias-assessment" aria-label="Permalink to &quot;Section C: Fairness and Bias Assessment&quot;">​</a></h3><p><strong>C1. Defining Fairness for This Context</strong></p><p><em>What does &quot;fairness&quot; mean for this specific use case?</em></p><p><strong>Fairness Criteria Selected:</strong></p><ul><li>[ ] <strong>Individual fairness</strong>: Similar individuals receive similar outcomes</li><li>[ ] <strong>Group fairness</strong>: Different demographic groups have similar outcomes</li><li>[ ] <strong>Equal opportunity</strong>: Equal true positive rates across groups</li><li>[ ] <strong>Equalized odds</strong>: Equal true positive AND false positive rates across groups</li><li>[ ] <strong>Demographic parity</strong>: Equal selection rates across groups</li><li>[ ] <strong>Predictive parity</strong>: Equal precision (positive predictive value) across groups</li><li>[ ] <strong>Calibration</strong>: Predicted probabilities match actual outcomes across groups</li><li>[ ] Other: _____________________________</li></ul><p><em>Justification for selected fairness criteria:</em> [Explain why these criteria are appropriate for this context]</p><p><em>Note: Different fairness criteria may conflict; trade-offs must be explicitly considered.</em></p><p><strong>C2. Protected Characteristics and Sensitive Attributes</strong></p><p><em>Which characteristics are legally protected or sensitive in your jurisdiction?</em></p><ul><li>[ ] Race / Ethnicity</li><li>[ ] Gender / Sex</li><li>[ ] Age</li><li>[ ] Disability</li><li>[ ] Religion</li><li>[ ] National origin / Immigration status</li><li>[ ] Sexual orientation</li><li>[ ] Socioeconomic status</li><li>[ ] Geographic location (as proxy for other characteristics)</li><li>[ ] Other: _____________________________</li></ul><p><strong>Use of Protected Characteristics:</strong></p><ul><li>Are protected characteristics used as model inputs? [ ] Yes [ ] No [ ] Indirectly (through proxies)</li><li>If yes, provide legal justification: _____________________________</li><li>If no direct use, are there proxy variables that correlate with protected characteristics? <ul><li>[ ] Yes - List proxies: _____________________________</li><li>[ ] No</li></ul></li></ul><p><strong>C3. Bias Testing Results</strong></p><p><em>For each protected characteristic, test for bias:</em></p><p><strong>Test 1: Disparate Impact Analysis</strong></p><table tabindex="0"><thead><tr><th>Protected Group</th><th>Selection Rate</th><th>Ratio to Baseline</th><th>80% Rule Pass?</th><th>Assessment</th></tr></thead><tbody><tr><td>Male (baseline)</td><td>45%</td><td>1.00</td><td>N/A</td><td>Baseline</td></tr><tr><td>Female</td><td>38%</td><td>0.84</td><td>Yes (&gt;0.80)</td><td>Acceptable disparity</td></tr><tr><td>Age &lt;25</td><td>25%</td><td>0.56</td><td>No (&lt;0.80)</td><td>Adverse impact detected</td></tr><tr><td>Age 25-50</td><td>48%</td><td>1.07</td><td>Yes</td><td>Acceptable</td></tr><tr><td>Age 50+</td><td>42%</td><td>0.93</td><td>Yes</td><td>Acceptable</td></tr></tbody></table><p><em>80% Rule: Selection rate for protected group should be at least 80% of the rate for the baseline group.</em></p><p><strong>Test 2: Confusion Matrix by Group</strong></p><p><em>Example for Gender:</em></p><table tabindex="0"><thead><tr><th>Metric</th><th>Male</th><th>Female</th><th>Disparity</th></tr></thead><tbody><tr><td>True Positive Rate (TPR)</td><td>85%</td><td>78%</td><td>7 percentage points</td></tr><tr><td>False Positive Rate (FPR)</td><td>10%</td><td>15%</td><td>5 percentage points</td></tr><tr><td>True Negative Rate (TNR)</td><td>90%</td><td>85%</td><td>5 percentage points</td></tr><tr><td>False Negative Rate (FNR)</td><td>15%</td><td>22%</td><td>7 percentage points</td></tr></tbody></table><p><em>Repeat for each protected characteristic.</em></p><p><strong>Test 3: Statistical Significance</strong></p><ul><li>Chi-square test p-value: _____</li><li>Statistical significance: [ ] Yes [ ] No</li><li>Practical significance: [ ] Yes [ ] No</li></ul><p><strong>C4. Root Cause Analysis of Bias</strong></p><p><em>For any detected bias, investigate root causes:</em></p><p><strong>Source of Bias:</strong></p><ul><li>[ ] Training data bias (historical discrimination reflected in data)</li><li>[ ] Sample bias (non-representative training data)</li><li>[ ] Measurement bias (how variables are measured)</li><li>[ ] Label bias (subjective or biased ground truth labels)</li><li>[ ] Algorithmic bias (model systematically disadvantages groups)</li><li>[ ] Proxy discrimination (correlated variables encode protected characteristics)</li><li>[ ] Feedback loops (algorithmic decisions reinforce existing patterns)</li></ul><p><em>Detailed analysis:</em> [Explain the mechanisms by which bias enters the system]</p><p><strong>C5. Intersectional Analysis</strong></p><p><em>Does the system have compounding effects for individuals with multiple protected characteristics?</em></p><p><em>Example: Performance for young, female, minority individuals</em></p><table tabindex="0"><thead><tr><th>Intersectional Group</th><th>Performance Metric</th><th>Compared to Overall</th><th>Assessment</th></tr></thead><tbody><tr><td>Young + Female</td><td>72% accuracy</td><td>-13 points</td><td>Significant disparity</td></tr><tr><td>Young + Minority</td><td>70% accuracy</td><td>-15 points</td><td>Significant disparity</td></tr><tr><td>Female + Minority</td><td>75% accuracy</td><td>-10 points</td><td>Moderate disparity</td></tr><tr><td>Young + Female + Minority</td><td>68% accuracy</td><td>-17 points</td><td>Severe disparity</td></tr></tbody></table><hr><h3 id="section-d-impact-assessment" tabindex="-1">Section D: Impact Assessment <a class="header-anchor" href="#section-d-impact-assessment" aria-label="Permalink to &quot;Section D: Impact Assessment&quot;">​</a></h3><p><strong>D1. Individual Impacts</strong></p><p><em>What are the potential impacts on individuals?</em></p><p><strong>Positive Impacts:</strong></p><ul><li>Faster processing times</li><li>More consistent decisions</li><li>Reduced human error</li><li>Improved access to services</li><li>[Other benefits]</li></ul><p><strong>Negative Impacts:</strong></p><table tabindex="0"><thead><tr><th>Impact Type</th><th>Description</th><th>Severity (1-5)</th><th>Affected Population</th><th>Frequency</th></tr></thead><tbody><tr><td>Economic harm</td><td>Denial of benefit reduces income</td><td>4</td><td>Low-income applicants</td><td>15% of denials</td></tr><tr><td>Reputational harm</td><td>False fraud flag impacts credit</td><td>3</td><td>All applicants</td><td>2% false positive rate</td></tr><tr><td>Psychological harm</td><td>Stress from automated rejection</td><td>2</td><td>All applicants</td><td>25% denied</td></tr><tr><td>Dignitary harm</td><td>Feeling dehumanized by automated process</td><td>2</td><td>All applicants</td><td>Unknown</td></tr><tr><td>Opportunity cost</td><td>Delayed application processing</td><td>1</td><td>All applicants</td><td>Rare</td></tr></tbody></table><p><em>Severity Scale: 1=Negligible, 2=Minor, 3=Moderate, 4=Major, 5=Severe</em></p><p><strong>D2. Vulnerable Populations</strong></p><p><em>Are vulnerable populations disproportionately impacted?</em></p><table tabindex="0"><thead><tr><th>Vulnerable Group</th><th>Specific Vulnerabilities</th><th>Disproportionate Impact?</th><th>Mitigation</th></tr></thead><tbody><tr><td>Low-income individuals</td><td>Limited ability to absorb economic harm</td><td>Yes - higher denial rate</td><td>Enhanced human review for borderline cases</td></tr><tr><td>Elderly</td><td>Less digital literacy, difficulty with appeals</td><td>Yes - lower successful appeal rate</td><td>Dedicated support hotline</td></tr><tr><td>People with disabilities</td><td>Accessibility barriers</td><td>Potentially</td><td>Ensure WCAG 2.1 AA compliance</td></tr><tr><td>Racial/ethnic minorities</td><td>Historical discrimination</td><td>Yes - detected bias</td><td>Bias mitigation (see Section E)</td></tr><tr><td>Non-native language speakers</td><td>Language barriers</td><td>Yes - lower understanding of process</td><td>Multilingual explanations</td></tr></tbody></table><p><strong>D3. Community and Societal Impacts</strong></p><p><em>Broader impacts beyond individuals:</em></p><p><strong>Community Cohesion:</strong></p><ul><li>Does the system affect trust between communities and government?</li><li>Could it exacerbate existing social divides?</li></ul><p>[Your response here]</p><p><strong>Democratic Values:</strong></p><ul><li>Does the system affect civic participation or freedom of expression?</li><li>Are there implications for due process or equal treatment under law?</li></ul><p>[Your response here]</p><p><strong>Labor Market:</strong></p><ul><li>How many jobs are affected by automation?</li><li>What is the plan for workforce transition?</li></ul><p>[Your response here]</p><p><strong>Environmental Impact:</strong></p><ul><li>What is the carbon footprint of training and running the model?</li><li>Is this justified by the benefits?</li></ul><p>[Your response here]</p><p><strong>D4. Error Analysis</strong></p><p><em>What happens when the system makes mistakes?</em></p><p><strong>False Positives (Type I Errors):</strong></p><ul><li>Definition: [What constitutes a false positive in this context]</li><li>Consequence for individual: [Impact]</li><li>Frequency: _____ % of positive predictions</li><li>Correction mechanism: [How errors are identified and corrected]</li></ul><p><strong>False Negatives (Type II Errors):</strong></p><ul><li>Definition: [What constitutes a false negative]</li><li>Consequence for individual: [Impact]</li><li>Frequency: _____ % of negative predictions</li><li>Correction mechanism: [How errors are identified and corrected]</li></ul><p><strong>Error Correction Process:</strong></p><ol><li>[Step 1]</li><li>[Step 2]</li><li>[Step 3]</li></ol><p><strong>Average Time to Correct Error:</strong> _____ days</p><hr><h3 id="section-e-transparency-and-explainability" tabindex="-1">Section E: Transparency and Explainability <a class="header-anchor" href="#section-e-transparency-and-explainability" aria-label="Permalink to &quot;Section E: Transparency and Explainability&quot;">​</a></h3><p><strong>E1. Technical Documentation</strong></p><p><em>Is comprehensive technical documentation available?</em></p><ul><li>[ ] System architecture document</li><li>[ ] Data dictionary (all input variables defined)</li><li>[ ] Model training methodology</li><li>[ ] Validation and testing procedures</li><li>[ ] Performance benchmarks</li><li>[ ] Bias testing results</li><li>[ ] Version control and change log</li></ul><p><strong>E2. Explainability Approach</strong></p><p><em>How are decisions explained to affected individuals?</em></p><p><strong>Global Explainability (how the system works in general):</strong></p><ul><li>[ ] Plainlanguage description of factors considered</li><li>[ ] Relative importance of factors (feature importance)</li><li>[ ] Examples of typical cases</li><li>[ ] Statistical performance information</li></ul><p><strong>Individual Explainability (why a specific decision was made):</strong></p><ul><li>[ ] <strong>Feature attribution</strong>: Which input factors most influenced this decision?</li><li>[ ] <strong>Counterfactual explanation</strong>: What would need to change for a different outcome?</li><li>[ ] <strong>Example-based</strong>: Similar cases and their outcomes</li><li>[ ] <strong>Rule extraction</strong>: Simplified decision rules that approximate the model</li><li>[ ] <strong>Saliency maps</strong>: Visual highlighting of important regions (for image/text)</li></ul><p><em>Explainability method used:</em> [e.g., SHAP values, LIME, attention mechanisms, decision tree approximation]</p><p><em>Sample explanation provided to user:</em> [Insert example of actual explanation that would be shown]</p><p><strong>E3. User-Facing Transparency</strong></p><p><em>What information is provided to individuals?</em></p><p><strong>Before Decision:</strong></p><ul><li>[ ] Notice that automated decision-making will be used</li><li>[ ] Explanation of how the system works</li><li>[ ] Factors that will be considered</li><li>[ ] How to prepare a strong application</li><li>[ ] Rights regarding automated decisions</li></ul><p><strong>At Time of Decision:</strong></p><ul><li>[ ] Clear statement of the decision</li><li>[ ] Explanation of key factors in this specific decision</li><li>[ ] How to request human review</li><li>[ ] How to appeal or contest</li></ul><p><strong>Example Decision Notice:</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Your application has been [APPROVED/DENIED] based on automated review.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Key factors in this decision:</span></span>
<span class="line"><span>- Income level: [Value] - [How it influenced decision]</span></span>
<span class="line"><span>- Employment history: [Value] - [How it influenced decision]</span></span>
<span class="line"><span>- Credit score: [Value] - [How it influenced decision]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>You have the right to:</span></span>
<span class="line"><span>1. Request a human review of this decision (within 30 days)</span></span>
<span class="line"><span>2. Appeal this decision (within 60 days)</span></span>
<span class="line"><span>3. Access the information used to make this decision</span></span>
<span class="line"><span></span></span>
<span class="line"><span>To request a review or appeal: [Contact information and process]</span></span></code></pre></div><p><strong>E4. Public Transparency</strong></p><p><em>What information is published for public accountability?</em></p><ul><li>[ ] High-level description of the system</li><li>[ ] Purpose and legal basis</li><li>[ ] Types of decisions made</li><li>[ ] Volume of decisions (monthly/annually)</li><li>[ ] Overall accuracy and performance metrics</li><li>[ ] Disparate impact analysis results</li><li>[ ] Audit results and remediation actions</li><li>[ ] This Algorithmic Impact Assessment (summary)</li></ul><p><strong>Public Register Entry:</strong> [Link to public-facing information about this system]</p><hr><h3 id="section-f-human-oversight-and-governance" tabindex="-1">Section F: Human Oversight and Governance <a class="header-anchor" href="#section-f-human-oversight-and-governance" aria-label="Permalink to &quot;Section F: Human Oversight and Governance&quot;">​</a></h3><p><strong>F1. Human Oversight Mechanisms</strong></p><p><em>What human oversight is in place?</em></p><p><strong>Pre-Deployment:</strong></p><ul><li>[ ] Expert review of model before launch</li><li>[ ] Testing with diverse scenarios</li><li>[ ] Stakeholder consultation</li><li>[ ] Ethics board approval</li></ul><p><strong>Ongoing:</strong></p><ul><li>[ ] Regular performance monitoring (frequency: _______)</li><li>[ ] Audit of decisions (sample size: _____, frequency: _____)</li><li>[ ] User feedback collection and analysis</li><li>[ ] Continuous bias monitoring</li></ul><p><strong>Human-in-the-Loop:</strong></p><ul><li>[ ] All decisions reviewed by human</li><li>[ ] High-risk or borderline decisions reviewed</li><li>[ ] Random sample reviewed (_____ %)</li><li>[ ] Flagged cases reviewed</li></ul><p><em>Human reviewer qualifications:</em> [Required training, expertise, experience]</p><p><em>Can human reviewers override algorithmic decisions?</em></p><ul><li>[ ] Yes, freely</li><li>[ ] Yes, with justification</li><li>[ ] No</li></ul><p><strong>F2. Appeal and Redress Mechanisms</strong></p><p><em>How can individuals challenge decisions?</em></p><p><strong>Request for Human Review:</strong></p><ul><li>Eligibility: [Who can request]</li><li>Process: [How to request]</li><li>Timeline: [How long review takes]</li><li>Outcome: [Possible results]</li></ul><p><strong>Formal Appeal:</strong></p><ul><li>Eligibility: [Who can appeal]</li><li>Grounds for appeal: [Valid reasons]</li><li>Evidence allowed: [What can be submitted]</li><li>Decision-maker: [Who reviews appeals]</li><li>Timeline: _____ days</li><li>Success rate: _____ % of appeals granted</li></ul><p><strong>Alternative Dispute Resolution:</strong></p><ul><li>[ ] Ombudsman review available</li><li>[ ] Mediation offered</li><li>[ ] External review board</li><li>[ ] Judicial review</li></ul><p><strong>Feedback Loop:</strong></p><ul><li>Are successful appeals used to improve the algorithm? [ ] Yes [ ] No</li><li>How: _____________________________</li></ul><p><strong>F3. Accountability Structure</strong></p><p><strong>Ownership and Responsibility:</strong></p><ul><li>System Owner: [Name, Role]</li><li>Accountable for overall performance and fairness</li><li>Authority to halt or modify system if issues arise</li></ul><p><strong>Governance Committee:</strong></p><ul><li>Membership: [Roles represented]</li><li>Meeting Frequency: [How often]</li><li>Responsibilities: [Oversight functions]</li></ul><p><strong>External Oversight:</strong></p><ul><li>[ ] Parliamentary/legislative oversight</li><li>[ ] Independent regulator</li><li>[ ] Audit by external experts</li><li>[ ] Public advisory committee</li><li>[ ] None</li></ul><p><strong>Incident Response:</strong></p><ul><li>[ ] Incident response plan exists</li><li>[ ] Clear escalation procedures</li><li>[ ] Defined thresholds for halting system</li></ul><p><em>Conditions that would trigger system suspension:</em></p><ol><li>[Condition]</li><li>[Condition]</li><li>[Condition]</li></ol><hr><h3 id="section-g-risk-mitigation-and-controls" tabindex="-1">Section G: Risk Mitigation and Controls <a class="header-anchor" href="#section-g-risk-mitigation-and-controls" aria-label="Permalink to &quot;Section G: Risk Mitigation and Controls&quot;">​</a></h3><p><strong>G1. Bias Mitigation Strategies</strong></p><p><em>How will identified biases be addressed?</em></p><p><strong>Pre-Processing (Data Level):</strong></p><ul><li>[ ] Collect more representative data</li><li>[ ] Re-weighting or re-sampling to balance groups</li><li>[ ] Synthetic data generation for under-represented groups</li><li>[ ] Remove or transform biased features</li></ul><p><strong>In-Processing (Algorithm Level):</strong></p><ul><li>[ ] Fairness constraints in model training</li><li>[ ] Adversarial debiasing</li><li>[ ] Calibrated equalized odds</li><li>[ ] Multi-objective optimization (accuracy + fairness)</li></ul><p><strong>Post-Processing (Decision Level):</strong></p><ul><li>[ ] Adjust decision thresholds by group</li><li>[ ] Re-ranking to equalize outcomes</li><li>[ ] Human review of borderline cases</li></ul><p><em>Selected mitigation approach:</em> [Describe specific methods implemented]</p><p><em>Expected impact of mitigation:</em></p><ul><li>Reduction in bias: _____ percentage points</li><li>Trade-off with accuracy: _____ percentage points</li><li>Justification for trade-off: _____________________________</li></ul><p><strong>G2. Performance Monitoring</strong></p><p><em>Ongoing monitoring to detect performance degradation or emerging bias:</em></p><p><strong>Metrics Tracked:</strong></p><ul><li>[ ] Overall accuracy</li><li>[ ] Accuracy by demographic group</li><li>[ ] False positive/negative rates</li><li>[ ] Disparate impact ratios</li><li>[ ] User appeals and outcomes</li><li>[ ] System errors and incidents</li></ul><p><strong>Monitoring Frequency:</strong></p><ul><li>Real-time: [Which metrics]</li><li>Daily: [Which metrics]</li><li>Weekly: [Which metrics]</li><li>Monthly: [Which metrics]</li><li>Quarterly: [Which metrics]</li></ul><p><strong>Alerting Thresholds:</strong></p><table tabindex="0"><thead><tr><th>Metric</th><th>Threshold</th><th>Alert Recipient</th><th>Action</th></tr></thead><tbody><tr><td>Accuracy drops below 80%</td><td>&lt;80%</td><td>System Owner</td><td>Investigate within 24 hours</td></tr><tr><td>Disparate impact ratio</td><td>&lt;0.80</td><td>Fairness Lead</td><td>Review within 1 week</td></tr><tr><td>Appeal rate increases</td><td>&gt;10%</td><td>System Owner</td><td>Root cause analysis</td></tr></tbody></table><p><strong>G3. Model Retraining and Updates</strong></p><p><em>How and when is the model updated?</em></p><p><strong>Retraining Schedule:</strong></p><ul><li>Frequency: [Monthly, Quarterly, Annually]</li><li>Trigger events: [Data drift, performance degradation, policy change]</li></ul><p><strong>Retraining Process:</strong></p><ol><li>[Step]</li><li>[Step]</li><li>[Step]</li></ol><p><strong>Re-Assessment Requirements:</strong></p><ul><li>[ ] Full AIA required for major model changes</li><li>[ ] Abbreviated AIA for minor updates</li><li>[ ] Performance testing required before deployment</li><li>[ ] Bias testing required before deployment</li><li>[ ] Approval by [Role] required</li></ul><p><strong>Version Control:</strong></p><ul><li>Model versions tracked: [ ] Yes [ ] No</li><li>Ability to roll back to previous version: [ ] Yes [ ] No</li><li>Documentation of changes between versions: [ ] Yes [ ] No</li></ul><p><strong>G4. Data Governance</strong></p><p><em>Controls to ensure data quality and integrity:</em></p><p><strong>Data Quality Checks:</strong></p><ul><li>[ ] Validation rules for input data</li><li>[ ] Automated data quality monitoring</li><li>[ ] Regular data audits</li><li>[ ] Error correction procedures</li></ul><p><strong>Data Provenance:</strong></p><ul><li>[ ] Source of all data documented</li><li>[ ] Chain of custody tracked</li><li>[ ] Audit trail of data transformations</li></ul><p><strong>Data Security:</strong></p><ul><li>[ ] Encryption in transit and at rest</li><li>[ ] Access controls (role-based)</li><li>[ ] Audit logging of data access</li><li>[ ] Secure data deletion procedures</li></ul><hr><h3 id="section-h-stakeholder-engagement" tabindex="-1">Section H: Stakeholder Engagement <a class="header-anchor" href="#section-h-stakeholder-engagement" aria-label="Permalink to &quot;Section H: Stakeholder Engagement&quot;">​</a></h3><p><strong>H1. Consultation Process</strong></p><p><em>Who was consulted during the AIA?</em></p><table tabindex="0"><thead><tr><th>Stakeholder Group</th><th>Engagement Method</th><th>Date</th><th>Key Feedback</th><th>How Addressed</th></tr></thead><tbody><tr><td>Affected individuals</td><td>Focus groups</td><td>[Date]</td><td>[Summary]</td><td>[Actions taken]</td></tr><tr><td>Civil society organizations</td><td>Public consultation</td><td>[Date]</td><td>[Summary]</td><td>[Actions taken]</td></tr><tr><td>Technical experts</td><td>Expert panel</td><td>[Date]</td><td>[Summary]</td><td>[Actions taken]</td></tr><tr><td>Ethics board</td><td>Formal review</td><td>[Date]</td><td>[Summary]</td><td>[Actions taken]</td></tr><tr><td>Academic researchers</td><td>Written submission</td><td>[Date]</td><td>[Summary]</td><td>[Actions taken]</td></tr></tbody></table><p><strong>H2. Ongoing Engagement</strong></p><p><em>How will stakeholders be engaged after deployment?</em></p><ul><li>[ ] User feedback mechanism (surveys, hotline)</li><li>[ ] Regular advisory committee meetings</li><li>[ ] Annual public reporting</li><li>[ ] Crowdsourced bias detection</li><li>[ ] Open challenge or red-teaming events</li><li>[ ] Other: _____________________________</li></ul><p><strong>H3. Communication and Education</strong></p><p><em>How will the system be explained to different audiences?</em></p><p><strong>For Affected Individuals:</strong></p><ul><li>Plain-language guides</li><li>FAQ documents</li><li>Video explanations</li><li>Chatbot assistance</li></ul><p><strong>For Front-Line Staff:</strong></p><ul><li>Training programs</li><li>Standard operating procedures</li><li>Decision support tools</li><li>Escalation guidelines</li></ul><p><strong>For General Public:</strong></p><ul><li>Website information</li><li>Media releases</li><li>Public presentations</li><li>Transparency reports</li></ul><hr><h3 id="section-i-testing-and-validation" tabindex="-1">Section I: Testing and Validation <a class="header-anchor" href="#section-i-testing-and-validation" aria-label="Permalink to &quot;Section I: Testing and Validation&quot;">​</a></h3><p><strong>I1. Pre-Deployment Testing</strong></p><p><em>What testing was conducted before launch?</em></p><p><strong>Functionality Testing:</strong></p><ul><li>[ ] Unit tests for code components</li><li>[ ] Integration tests for system components</li><li>[ ] End-to-end workflow tests</li><li>[ ] Edge case and boundary condition tests</li></ul><p><strong>Performance Testing:</strong></p><ul><li>[ ] Accuracy on holdout test set</li><li>[ ] Cross-validation across multiple datasets</li><li>[ ] Stress testing (high volume)</li><li>[ ] Latency and response time testing</li></ul><p><strong>Fairness Testing:</strong></p><ul><li>[ ] Disparate impact analysis (see Section C)</li><li>[ ] Equalized odds evaluation</li><li>[ ] Calibration testing across groups</li><li>[ ] Intersectional fairness analysis</li></ul><p><strong>Adversarial Testing:</strong></p><ul><li>[ ] Robustness to adversarial examples</li><li>[ ] Gaming or manipulation attempts</li><li>[ ] Data poisoning scenarios</li><li>[ ] Privacy attacks (membership inference, model inversion)</li></ul><p><strong>User Acceptance Testing:</strong></p><ul><li>[ ] Usability testing with representative users</li><li>[ ] Explanation comprehension testing</li><li>[ ] Appeal process simulation</li><li>[ ] Satisfaction surveys</li></ul><p><strong>I2. Pilot or Limited Deployment</strong></p><p><em>Was the system piloted before full deployment?</em></p><ul><li>[ ] Yes - Details: _____________________________</li><li>[ ] No - Justification: _____________________________</li></ul><p><em>If yes:</em></p><ul><li>Pilot scope: _____________________________</li><li>Duration: _____________________________</li><li>Findings: _____________________________</li><li>Changes made based on pilot: _____________________________</li></ul><p><strong>I3. Ongoing Validation</strong></p><p><em>How will the system be validated in production?</em></p><ul><li>[ ] A/B testing against baseline (human decisions or previous system)</li><li>[ ] Regular audits of decision sample</li><li>[ ] Comparison of predicted vs. actual outcomes</li><li>[ ] User satisfaction measurement</li><li>[ ] Independent external audit (frequency: _______)</li></ul><hr><h3 id="section-j-legal-and-ethical-compliance" tabindex="-1">Section J: Legal and Ethical Compliance <a class="header-anchor" href="#section-j-legal-and-ethical-compliance" aria-label="Permalink to &quot;Section J: Legal and Ethical Compliance&quot;">​</a></h3><p><strong>J1. Legal Compliance</strong></p><p><em>Compliance with applicable laws and regulations:</em></p><table tabindex="0"><thead><tr><th>Legal Requirement</th><th>How Compliance Achieved</th><th>Evidence</th></tr></thead><tbody><tr><td>Non-discrimination laws</td><td>Bias testing and mitigation</td><td>AIA Section C</td></tr><tr><td>Data protection/privacy</td><td>PIA completed; consent/legal basis</td><td>PIA reference: _____</td></tr><tr><td>Transparency requirements</td><td>Public disclosure; individual explanations</td><td>AIA Section E</td></tr><tr><td>Right to human review</td><td>Appeal mechanisms established</td><td>AIA Section F</td></tr><tr><td>Algorithmic accountability regulations</td><td>This AIA; governance structure</td><td>AIA Section F</td></tr><tr><td>Sector-specific regulations</td><td>[Specific compliance measures]</td><td>[Evidence]</td></tr></tbody></table><p><strong>J2. Ethical Principles</strong></p><p><em>Alignment with ethical AI principles:</em></p><p><strong>Beneficence (Do Good):</strong> [How the system provides public benefit]</p><p><strong>Non-Maleficence (Do No Harm):</strong> [How potential harms are minimized]</p><p><strong>Autonomy:</strong> [How individual agency and choice are preserved]</p><p><strong>Justice:</strong> [How fairness and equitable treatment are ensured]</p><p><strong>Explicability:</strong> [How transparency and explainability are provided]</p><p><strong>J3. Rights and Safeguards</strong></p><p><em>Individual rights protected:</em></p><ul><li>[ ] Right to be informed</li><li>[ ] Right to human review</li><li>[ ] Right to explanation</li><li>[ ] Right to appeal</li><li>[ ] Right to rectification</li><li>[ ] Right to erasure (where applicable)</li><li>[ ] Right to non-discrimination</li></ul><hr><h3 id="section-k-approval-and-publication" tabindex="-1">Section K: Approval and Publication <a class="header-anchor" href="#section-k-approval-and-publication" aria-label="Permalink to &quot;Section K: Approval and Publication&quot;">​</a></h3><p><strong>K1. Review and Approval</strong></p><p><strong>Ethics Board Review:</strong></p><ul><li>Review Date: _____________________________</li><li>Recommendation: <ul><li>[ ] Approved as proposed</li><li>[ ] Approved with conditions (specify): _____________________________</li><li>[ ] Requires modification</li><li>[ ] Not approved</li></ul></li><li>Ethics Board Chair Signature: _____________________________</li></ul><p><strong>Data Protection Officer Review:</strong></p><ul><li>Review Date: _____________________________</li><li>Privacy compliance confirmed: [ ] Yes [ ] No</li><li>Concerns: _____________________________</li><li>DPO Signature: _____________________________</li></ul><p><strong>Legal Counsel Review:</strong></p><ul><li>Review Date: _____________________________</li><li>Legal compliance confirmed: [ ] Yes [ ] No</li><li>Concerns: _____________________________</li><li>Legal Counsel Signature: _____________________________</li></ul><p><strong>Senior Management Approval:</strong></p><ul><li>Approving Official Name and Title: _____________________________</li><li>Approval Date: _____________________________</li><li>Signature: _____________________________</li></ul><p><strong>K2. Public Summary for Transparency</strong></p><p><em>Complete this section for publication (redact sensitive technical details if necessary):</em></p><p><strong>System Name:</strong> _____________________________</p><p><strong>Purpose:</strong> [Brief description]</p><p><strong>Decisions Made:</strong> [Type and volume]</p><p><strong>Algorithmic Approach:</strong> [High-level description]</p><p><strong>Fairness Assessment:</strong> [Summary of bias testing and mitigation]</p><p><strong>Accuracy:</strong> [Overall performance]</p><p><strong>Human Oversight:</strong> [Description of review mechanisms]</p><p><strong>Appeal Rights:</strong> [How to challenge decisions]</p><p><strong>Approval Date:</strong> _____________________________</p><p><strong>Contact for Inquiries:</strong> _____________________________</p><p><strong>Full AIA Available:</strong> [Yes/No - If no, explain why]</p><hr><h3 id="section-l-ongoing-management-and-review" tabindex="-1">Section L: Ongoing Management and Review <a class="header-anchor" href="#section-l-ongoing-management-and-review" aria-label="Permalink to &quot;Section L: Ongoing Management and Review&quot;">​</a></h3><p><strong>L1. Monitoring and Reporting</strong></p><p><strong>Performance Dashboard:</strong></p><ul><li>[ ] Real-time dashboard accessible to system owner</li><li>[ ] Monthly reports to management</li><li>[ ] Quarterly reports to governance committee</li><li>[ ] Annual public transparency report</li></ul><p><strong>L2. Review Schedule</strong></p><p><strong>Regular Reviews:</strong></p><ul><li>Quarterly: Performance and fairness metrics</li><li>Annually: Full AIA update</li><li>Upon: Major system changes, significant incidents, regulatory changes</li></ul><p><strong>Next Review Date:</strong> _____________________________</p><p><strong>L3. Continuous Improvement</strong></p><p><em>How will lessons learned be incorporated?</em></p><ul><li>[ ] Regular retrospectives</li><li>[ ] User feedback integration</li><li>[ ] Benchmarking against best practices</li><li>[ ] Participation in AI ethics community of practice</li></ul><p><strong>L4. Decommissioning Plan</strong></p><p><em>What happens if the system is discontinued?</em></p><ul><li>Data retention/deletion: _____________________________</li><li>Transition plan for affected individuals: _____________________________</li><li>Documentation archiving: _____________________________</li></ul><hr><h2 id="part-3-supporting-resources" tabindex="-1">Part 3: Supporting Resources <a class="header-anchor" href="#part-3-supporting-resources" aria-label="Permalink to &quot;Part 3: Supporting Resources&quot;">​</a></h2><h3 id="appendix-a-fairness-metrics-reference" tabindex="-1">Appendix A: Fairness Metrics Reference <a class="header-anchor" href="#appendix-a-fairness-metrics-reference" aria-label="Permalink to &quot;Appendix A: Fairness Metrics Reference&quot;">​</a></h3><p><strong>Common Fairness Definitions:</strong></p>`,377),e("ol",null,[e("li",null,[e("p",null,[e("strong",null,"Demographic Parity"),i(": P(Ŷ=1|A=0) = P(Ŷ=1|A=1)")]),e("ul",null,[e("li",null,"Equal selection rates across groups")])]),e("li",{"0,1":""},[e("p",null,[e("strong",null,"Equalized Odds"),i(": P(Ŷ=1|Y=y,A=0) = P(Ŷ=1|Y=y,A=1) for y ∈")]),e("ul",null,[e("li",null,"Equal true positive AND false positive rates")])]),e("li",null,[e("p",null,[e("strong",null,"Equal Opportunity"),i(": P(Ŷ=1|Y=1,A=0) = P(Ŷ=1|Y=1,A=1)")]),e("ul",null,[e("li",null,"Equal true positive rates (recall)")])]),e("li",null,[e("p",null,[e("strong",null,"Predictive Parity"),i(": P(Y=1|Ŷ=1,A=0) = P(Y=1|Ŷ=1,A=1)")]),e("ul",null,[e("li",null,"Equal precision across groups")])]),e("li",null,[e("p",null,[e("strong",null,"Calibration"),i(": P(Y=1|Ŷ=p,A=a) = p for all p, a")]),e("ul",null,[e("li",null,"Predicted probabilities match actual frequencies")])])],-1),a('<p><em>Note: These definitions may be mathematically incompatible; trade-offs are necessary.</em></p><h3 id="appendix-b-bias-mitigation-techniques-reference" tabindex="-1">Appendix B: Bias Mitigation Techniques Reference <a class="header-anchor" href="#appendix-b-bias-mitigation-techniques-reference" aria-label="Permalink to &quot;Appendix B: Bias Mitigation Techniques Reference&quot;">​</a></h3><p><strong>Pre-Processing:</strong></p><ul><li>Re-weighting samples</li><li>Disparate impact remover</li><li>Learning fair representations</li><li>Synthetic minority over-sampling (SMOTE)</li></ul><p><strong>In-Processing:</strong></p><ul><li>Adversarial debiasing</li><li>Prejudice remover regularization</li><li>Fairness constraints optimization</li><li>Meta-fair classifier</li></ul><p><strong>Post-Processing:</strong></p><ul><li>Equalized odds post-processing</li><li>Calibrated equalized odds</li><li>Reject option classification</li><li>Threshold optimization</li></ul><h3 id="appendix-c-explainability-methods-reference" tabindex="-1">Appendix C: Explainability Methods Reference <a class="header-anchor" href="#appendix-c-explainability-methods-reference" aria-label="Permalink to &quot;Appendix C: Explainability Methods Reference&quot;">​</a></h3><p><strong>Model-Agnostic:</strong></p><ul><li>LIME (Local Interpretable Model-Agnostic Explanations)</li><li>SHAP (SHapley Additive exPlanations)</li><li>Anchors (rule-based explanations)</li><li>Counterfactual explanations</li></ul><p><strong>Model-Specific:</strong></p><ul><li>Decision tree visualization</li><li>Feature importance (tree-based models)</li><li>Attention mechanisms (neural networks)</li><li>Saliency maps (computer vision)</li></ul><h3 id="appendix-d-related-frameworks-and-standards" tabindex="-1">Appendix D: Related Frameworks and Standards <a class="header-anchor" href="#appendix-d-related-frameworks-and-standards" aria-label="Permalink to &quot;Appendix D: Related Frameworks and Standards&quot;">​</a></h3><p><strong>International Standards:</strong></p><ul><li>ISO/IEC 42001 - AI Management System</li><li>IEEE 7000 series - Ethically aligned design</li><li>NIST AI Risk Management Framework</li><li>EU AI Act requirements</li></ul><p><strong>Best Practice Frameworks:</strong></p><ul><li>Algorithmic Accountability Act (proposed, US)</li><li>UK ICO Guidance on AI and Data Protection</li><li>Canadian Algorithmic Impact Assessment</li><li>Australian AI Ethics Framework</li></ul><h3 id="appendix-e-resources-and-contacts" tabindex="-1">Appendix E: Resources and Contacts <a class="header-anchor" href="#appendix-e-resources-and-contacts" aria-label="Permalink to &quot;Appendix E: Resources and Contacts&quot;">​</a></h3><p><strong>Technical Support:</strong></p><ul><li>AI Ethics Team: ai-ethics@[agency].gov</li><li>Data Science Team: data-science@[agency].gov</li></ul><p><strong>Policy Guidance:</strong></p><ul><li>Algorithmic Accountability Office: algorithmic-accountability@[agency].gov</li><li>Data Protection Officer: dpo@[agency].gov</li></ul><p><strong>Public Inquiries:</strong></p><ul><li>General Information: aia-info@[agency].gov</li><li>Appeal Assistance: algorithmic-appeals@[agency].gov</li></ul><hr><p><strong>Document Control:</strong></p><ul><li><strong>Template ID:</strong> GaaS-T4-AIA-001</li><li><strong>Classification:</strong> Public</li><li><strong>Review Cycle:</strong> Annual</li><li><strong>Next Review:</strong> October 2026</li><li><strong>Owner:</strong> [National Digital Government Office]</li></ul><p><em>This template is part of the Government as a Service (GaaS) framework. It may be freely adapted for national, regional, or local government use. Attribution appreciated but not required.</em></p>',29)])])}const h=n(r,[["render",o]]);export{g as __pageData,h as default};
